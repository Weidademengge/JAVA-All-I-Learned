概念
智能体（agent）
状态（state）
行为（action）
奖励（reward）
策略（policy）

每一个动作（action）都能影响代理将来的状态（state）通过一个标量的奖励（reward）信号来衡量成功
目标：选择一系列行动来最大化未来的奖励
![[Pasted image 20230206191448.png]]
![[Pasted image 20230206191530.png]]
![[Pasted image 20230206191609.png]]

马尔可夫决策要求：
1、能够检测到理想的状态
2、可以多次尝试
3、系统的下个状态只与当前状态信息关系，而与更早之前的状态无关。在决策过程中还和当前采取的动作有关。

马尔科夫决策过程由5个元素构成：

s：表示状态集（states）
A：表示一组动作（actions）
P：表示状态转移概率Psa表示在当前s∈S状态下，经过a∈A动作后，会转移到的其他状态的概率分布情况在状态s下执行动作a，转移到s‘的概率可以表示为p(s'|s,a)
R:奖励函数（reward function）表示agent采取某个动作后的及时奖励
y：折扣系数意味着当下的reward比未来反馈的reward更重要
![[Pasted image 20230206214727.png]]

1.智能体初始状态为S0
2.选择一个动作a0
3.按概率转移矩阵Psa转移到了下一个状态S1

![[Pasted image 20230206214951.png]]

状态价值函数:
![[Pasted image 20230206215026.png]]

t时刻的状态s能获得的未来回报的期望
价值函数用来衡量某一个状态或状态-动作对的优劣价，累计奖励的期望

最有价值函数：所有策略下的最有累计奖励期望：
![[Pasted image 20230206215144.png]]

策略：已知状态下可能产生动作的概率分布

Bellman方程：当前状态的价值和下一步的价值及档期啊你的奖励（reward）有关价值函数分解为当前的奖励和下一步的价值两部分。
![[Pasted image 20230206215803.png]]